import logging
import os
from motor.motor_asyncio import AsyncIOMotorClient
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
from langchain_openai import ChatOpenAI, OpenAIEmbeddings # <--- Ð­Ð¢Ðž Ð’ÐÐ–ÐÐž
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð³ÐµÑ€Ð°
logger = logging.getLogger(__name__)


async def prepare_ragas_dataset(mongo_uri: str):
    """
    Ð‘Ð›ÐžÐš 1: ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
    Ð”Ð¾ÑÑ‚Ð°ÐµÐ¼ Ð»Ð¾Ð³Ð¸ ÑÐ¾ ÑÑ‚Ð°Ñ‚ÑƒÑÐ¾Ð¼ 'pending', Ñ‡Ð¸ÑÑ‚Ð¸Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Dataset.
    """
    client = AsyncIOMotorClient(mongo_uri)
    db = client.joytishai_db
    collection = db.ai_logs

    # 1. Ð‘ÐµÑ€ÐµÐ¼ 5 ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð»Ð¾Ð³Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐµÑ‰Ðµ Ð½Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐµÐ½Ñ‹
    cursor = collection.find({"evaluation.status": "pending"}).limit(5)
    logs = await cursor.to_list(length=5)

    if not logs:
        return None, None, collection

    # 2. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð»Ñ Ragas
    ragas_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }

    # 3. Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑÐ¿Ð¸ÑÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
    for log in logs:
        # --- Ð’Ð¾Ð¿Ñ€Ð¾Ñ ---
        ragas_data["question"].append(str(log.get("user_query", "")))

        # --- ÐžÑ‚Ð²ÐµÑ‚ AI ---
        response_obj = log.get("response", {})
        if isinstance(response_obj, dict):
            ans_text = response_obj.get("astrological_analysis", "")
        else:
            ans_text = str(response_obj)
        ragas_data["answer"].append(ans_text)

        # --- ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (ÐžÑ‡Ð¸ÑÑ‚ÐºÐ°) ---
        raw_context = log.get("context", [])


        print(f"ðŸ› RAW CONTEXT ID {log['_id']}: {raw_context}")

        cleaned_context = []

        # ðŸ‘‡ Ð”ÐžÐ‘ÐÐ’Ð¬ Ð­Ð¢Ð˜ Ð¡Ð¢Ð ÐžÐšÐ˜ Ð”Ð›Ð¯ ÐžÐ¢Ð›ÐÐ”ÐšÐ˜ ðŸ‘‡
        print(f"\nðŸ“¦ --- DEBUG ID: {log['_id']} ---")
        print(f"ðŸ”‘ ÐšÐ»ÑŽÑ‡Ð¸ Ð² Ð·Ð°Ð¿Ð¸ÑÐ¸: {list(log.keys())}")
        print(f"ðŸ“„ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¿Ð¾Ð»Ñ 'context': {log.get('context')}")
        print(f"ðŸ“„ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¿Ð¾Ð»Ñ 'metadata_context': {log.get('metadata_context')}")
        print("-----------------------------------\n")


        for item in raw_context:
            if isinstance(item, str):
                cleaned_context.append(item)
            elif isinstance(item, dict):
                # Ð˜Ñ‰ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° (page_content Ð¸Ð»Ð¸ text)
                text = item.get("page_content") or item.get("text") or str(item)
                cleaned_context.append(text)
            else:
                cleaned_context.append(str(item))

        ragas_data["contexts"].append(cleaned_context)

        # Ground Truth (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)
        ragas_data["ground_truth"].append("nan")

    # 4. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Dataset
    dataset = Dataset.from_dict(ragas_data)
    logger.info(f"ðŸ“Š ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½ Dataset Ð¸Ð· {len(logs)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")

    return dataset, logs, collection







async def execute_ragas_cycle(mongo_uri: str):
    """
    Ð‘Ð›ÐžÐš 2: Ð¯Ð´Ñ€Ð¾ Ð¾Ñ†ÐµÐ½ÐºÐ¸ (Evaluation Engine)
    Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ragas Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² MongoDB.
    """
    logger.info("ðŸ‘¨â€âš–ï¸ [RAGAS ENGINE] Ð—Ð°Ð¿ÑƒÑÐº Ñ†Ð¸ÐºÐ»Ð° Ð¾Ñ†ÐµÐ½ÐºÐ¸...")

    # 1. Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    dataset, logs, collection = await prepare_ragas_dataset(mongo_uri)

    if not dataset:
        logger.info("ðŸ’¤ ÐÐµÑ‚ Ð»Ð¾Ð³Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ (status='pending').")
        return {"status": "idle", "processed": 0}

    # 2. ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ ÑÑƒÐ´ÑŒÐ¸
    active_metrics = [
        faithfulness,  # ÐÐµ Ð²Ñ€ÐµÑ‚ Ð»Ð¸?
        answer_relevancy,  # ÐŸÐ¾ Ñ‚ÐµÐ¼Ðµ Ð»Ð¸?
        context_precision  # ÐšÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð»Ð¸ Ð¿Ð¾Ð¸ÑÐº?
    ]

    try:
        logger.info(f"â³ ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ {len(logs)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ ÑÑƒÐ´ÑŒÐµ Ragas...")

        # --- ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ Ð¡Ð£Ð”Ð¬Ð˜ ---
        # 1. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ (LLM)
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ gpt-4o Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸Ð»Ð¸ gpt-4o-mini Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸
        # --- ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ Ð¡Ð£Ð”Ð¬Ð˜ ---

        # 1. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ChatOpenAI Ð¸ Ð¾Ð±Ð¾Ñ€Ð°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐµÐ³Ð¾ Ð´Ð»Ñ Ragas
        judge_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o", temperature=0))

        # 2. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ OpenAIEmbeddings Ð¸ Ñ‚Ð¾Ð¶Ðµ Ð¾Ð±Ð¾Ñ€Ð°Ñ‡Ð¸Ð²Ð°ÐµÐ¼
        judge_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model="text-embedding-3-small"))



        print("\nðŸ” --- ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð”ÐÐÐÐ«Ð¥ Ð”Ð›Ð¯ RAGAS ---")
        print(f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {dataset['question'][0]}")
        print(f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ (Ñ‡Ñ‚Ð¾ Ð½Ð°ÑˆÐ»Ð¸): {dataset['contexts'][0]}")
        print(f"ÐžÑ‚Ð²ÐµÑ‚ (Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼): {dataset['answer'][0]}")
        print("---------------------------------------\n")


        # 3. ÐœÐÐ“Ð˜Ð¯ RAGAS ðŸš€
        results = evaluate(
            dataset=dataset,
            metrics=active_metrics,
            llm=judge_llm,  # <--- ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ ÑÐ²Ð½Ð¾
            embeddings=judge_embeddings  # <--- ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ ÑÐ²Ð½Ð¾
        )





        # 4. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        df = results.to_pandas()
        count = 0

        for i, log in enumerate(logs):
            scores = df.iloc[i]

            await collection.update_one(
                {"_id": log["_id"]},
                {
                    "$set": {
                        "evaluation.faithfulness": float(scores.get("faithfulness", 0)),
                        "evaluation.relevancy": float(scores.get("answer_relevancy", 0)),
                        "evaluation.context_precision": float(scores.get("context_precision", 0)),
                        "evaluation.status": "evaluated",
                        "evaluation.engine": "ragas_professional"
                    }
                }
            )
            count += 1
            print(f"âœ… Ragas ID: {log['_id']} | Faith: {scores.get('faithfulness'):.2f}")

        return {"status": "success", "processed": count}

    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ragas: {e}")
        # ÐœÐ°Ñ€ÐºÐ¸Ñ€ÑƒÐµÐ¼ Ð¾ÑˆÐ¸Ð±ÐºÑƒ Ð² Ð±Ð°Ð·Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð·Ð°Ñ†Ð¸ÐºÐ»Ð¸Ð²Ð°Ñ‚ÑŒÑÑ
        if logs:
            ids = [log["_id"] for log in logs]
            await collection.update_many(
                {"_id": {"$in": ids}},
                {"$set": {"evaluation.status": "error", "evaluation.error_msg": str(e)}}
            )
        return {"status": "error", "message": str(e)}