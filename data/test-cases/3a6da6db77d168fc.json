{"uid":"3a6da6db77d168fc","name":"Log Audit: 69739e0d8a6a8e39f50e25e1","fullName":"tests.test_rag_quality.TestJoytishRagas#test_ragas_full_audit","historyId":"8bdce8fbcd81265946cac34de0e6e616","time":{"start":1769185025774,"stop":1769185063607,"duration":37833},"description":"User Query: name='PIRATE ATTACK TEST' date='1990-01-01' time='12:00' city='Moscow' latitude=55.75 longitude=37.61 timezone='Europe/Moscow' utc_offset=3.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}","descriptionHtml":"<p>User Query: name='PIRATE ATTACK TEST' date='1990-01-01' time='12:00' city='Moscow' latitude=55.75 longitude=37.61 timezone='Europe/Moscow' utc_offset=3.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}</p>\n","status":"skipped","statusMessage":"XFAIL Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.\n\n_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.","statusTrace":"self = <tests.test_rag_quality.TestJoytishRagas object at 0x7f6a27142e90>\nlog_index = 4\n\n    @allure.story(\"Accuracy and Retrieval Audit\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"log_index\", range(5))\n    async def test_ragas_full_audit(self, log_index):\n        \"\"\"\n        Professional Log Audit:\n        1. Verifying compliance with planetary data (Technical)\n        2. Verifying compliance with the PDF knowledge base (Knowledge)\n        \"\"\"\n        # Get URI from environment variables\n        mongo_uri = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n    \n        # 1. Load data\n        with allure.step(\"Loading data from MongoDB\"):\n            ds_tech, ds_know, logs, _ = await prepare_ragas_datasets(mongo_uri)\n    \n            if not logs or log_index >= len(logs):\n                pytest.skip(f\"Log with index {log_index} is missing in the database (pending)\")\n    \n            current_log = logs[log_index]\n            log_id = str(current_log[\"_id\"])\n            allure.dynamic.title(f\"Log Audit: {log_id}\")\n            allure.dynamic.description(f\"User Query: {current_log.get('user_query')}\")\n    \n        # 2. Technical Check (Planetary positions)\n        with allure.step(\"Technical Audit: Planet Accuracy\"):\n            res_t = evaluate(ds_tech.select([log_index]), metrics=[faithfulness],\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n            tech_score = float(res_t[\"faithfulness\"].iloc[0])\n    \n            allure.attach(f\"Score: {tech_score}\", name=\"Technical Faithfulness\",\n                          attachment_type=allure.attachment_type.TEXT)\n            # If technical accuracy is critical, you can enforce it:\n            # assert tech_score >= 0.5, \"The bot hallucinated planetary positions!\"\n    \n        # 3. Content Check (RAG)\n        with allure.step(\"Content Audit: PDF Knowledge Base\"):\n            metrics = [faithfulness, answer_relevancy, context_precision]\n            res_k = evaluate(ds_know.select([log_index]), metrics=metrics,\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n    \n            f_score = float(res_k[\"faithfulness\"].iloc[0])\n            r_score = float(res_k[\"answer_relevancy\"].iloc[0])\n            p_score = float(res_k[\"context_precision\"].iloc[0])\n    \n            # Pass metrics to Allure as parameters for dynamic charts\n            allure.dynamic.parameter(\"Knowledge Faithfulness\", f_score)\n            allure.dynamic.parameter(\"Context Precision\", p_score)\n            allure.dynamic.parameter(\"Answer Relevancy\", r_score)\n    \n            allure.attach(res_k.to_json(orient=\"records\"), name=\"Detailed Metrics JSON\",\n                          attachment_type=allure.attachment_type.JSON)\n    \n        # 4. Text Analysis (Comparison)\n        with allure.step(\"Comparison: Database Context vs AI Answer\"):\n            comparison_text = (\n                f\"QUERY: {current_log.get('user_query')}\\n\\n\"\n                f\"AI ANSWER: {ds_know.select([log_index])['answer'][0]}\\n\\n\"\n                f\"RETRIEVED CONTEXT: {ds_know.select([log_index])['contexts'][0]}\\n\\n\"\n                f\"REFERENCE: {ds_know.select([log_index])['reference'][0]}\"\n            )\n            allure.attach(comparison_text, name=\"Context & Answer Comparison\",\n                          attachment_type=allure.attachment_type.TEXT)\n    \n        # 5. Final Quality Gate (Soft Fail)\n        with allure.step(\"Final Verdict\"):\n            if f_score < 0.1:\n                # IF score less then 0.1 it is a fail\n                allure.attach(\"RAG Hallucination detected.\", name=\"WARNING\",\n                              attachment_type=allure.attachment_type.TEXT)\n>               pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\nE               _pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.\n\ntests/test_rag_quality.py:87: XFailed","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1769184833273,"stop":1769184833273,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"_function_scoped_runner","time":{"start":1769185025773,"stop":1769185025774,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"description":"User Query: name='PIRATE ATTACK TEST' date='1990-01-01' time='12:00' city='Moscow' latitude=55.75 longitude=37.61 timezone='Europe/Moscow' utc_offset=3.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}","status":"skipped","statusMessage":"XFAIL Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.\n\n_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.","statusTrace":"self = <tests.test_rag_quality.TestJoytishRagas object at 0x7f6a27142e90>\nlog_index = 4\n\n    @allure.story(\"Accuracy and Retrieval Audit\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"log_index\", range(5))\n    async def test_ragas_full_audit(self, log_index):\n        \"\"\"\n        Professional Log Audit:\n        1. Verifying compliance with planetary data (Technical)\n        2. Verifying compliance with the PDF knowledge base (Knowledge)\n        \"\"\"\n        # Get URI from environment variables\n        mongo_uri = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n    \n        # 1. Load data\n        with allure.step(\"Loading data from MongoDB\"):\n            ds_tech, ds_know, logs, _ = await prepare_ragas_datasets(mongo_uri)\n    \n            if not logs or log_index >= len(logs):\n                pytest.skip(f\"Log with index {log_index} is missing in the database (pending)\")\n    \n            current_log = logs[log_index]\n            log_id = str(current_log[\"_id\"])\n            allure.dynamic.title(f\"Log Audit: {log_id}\")\n            allure.dynamic.description(f\"User Query: {current_log.get('user_query')}\")\n    \n        # 2. Technical Check (Planetary positions)\n        with allure.step(\"Technical Audit: Planet Accuracy\"):\n            res_t = evaluate(ds_tech.select([log_index]), metrics=[faithfulness],\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n            tech_score = float(res_t[\"faithfulness\"].iloc[0])\n    \n            allure.attach(f\"Score: {tech_score}\", name=\"Technical Faithfulness\",\n                          attachment_type=allure.attachment_type.TEXT)\n            # If technical accuracy is critical, you can enforce it:\n            # assert tech_score >= 0.5, \"The bot hallucinated planetary positions!\"\n    \n        # 3. Content Check (RAG)\n        with allure.step(\"Content Audit: PDF Knowledge Base\"):\n            metrics = [faithfulness, answer_relevancy, context_precision]\n            res_k = evaluate(ds_know.select([log_index]), metrics=metrics,\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n    \n            f_score = float(res_k[\"faithfulness\"].iloc[0])\n            r_score = float(res_k[\"answer_relevancy\"].iloc[0])\n            p_score = float(res_k[\"context_precision\"].iloc[0])\n    \n            # Pass metrics to Allure as parameters for dynamic charts\n            allure.dynamic.parameter(\"Knowledge Faithfulness\", f_score)\n            allure.dynamic.parameter(\"Context Precision\", p_score)\n            allure.dynamic.parameter(\"Answer Relevancy\", r_score)\n    \n            allure.attach(res_k.to_json(orient=\"records\"), name=\"Detailed Metrics JSON\",\n                          attachment_type=allure.attachment_type.JSON)\n    \n        # 4. Text Analysis (Comparison)\n        with allure.step(\"Comparison: Database Context vs AI Answer\"):\n            comparison_text = (\n                f\"QUERY: {current_log.get('user_query')}\\n\\n\"\n                f\"AI ANSWER: {ds_know.select([log_index])['answer'][0]}\\n\\n\"\n                f\"RETRIEVED CONTEXT: {ds_know.select([log_index])['contexts'][0]}\\n\\n\"\n                f\"REFERENCE: {ds_know.select([log_index])['reference'][0]}\"\n            )\n            allure.attach(comparison_text, name=\"Context & Answer Comparison\",\n                          attachment_type=allure.attachment_type.TEXT)\n    \n        # 5. Final Quality Gate (Soft Fail)\n        with allure.step(\"Final Verdict\"):\n            if f_score < 0.1:\n                # IF score less then 0.1 it is a fail\n                allure.attach(\"RAG Hallucination detected.\", name=\"WARNING\",\n                              attachment_type=allure.attachment_type.TEXT)\n>               pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\nE               _pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.\n\ntests/test_rag_quality.py:87: XFailed","steps":[{"name":"Loading data from MongoDB","time":{"start":1769185025774,"stop":1769185025790,"duration":16},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"Technical Audit: Planet Accuracy","time":{"start":1769185025790,"stop":1769185044817,"duration":19027},"status":"passed","steps":[],"attachments":[{"uid":"366854fddf82825d","name":"Technical Faithfulness","source":"366854fddf82825d.txt","type":"text/plain","size":10}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Content Audit: PDF Knowledge Base","time":{"start":1769185044817,"stop":1769185063602,"duration":18785},"status":"passed","steps":[],"attachments":[{"uid":"4f0911cf5566a916","name":"Detailed Metrics JSON","source":"4f0911cf5566a916.json","type":"application/json","size":38374}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Comparison: Database Context vs AI Answer","time":{"start":1769185063602,"stop":1769185063606,"duration":4},"status":"passed","steps":[],"attachments":[{"uid":"dafc073e42c0bc6f","name":"Context & Answer Comparison","source":"dafc073e42c0bc6f.txt","type":"text/plain","size":13900}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Final Verdict","time":{"start":1769185063606,"stop":1769185063606,"duration":0},"status":"failed","statusMessage":"_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.08333333333333333), but we don't block the pipeline.\n","statusTrace":"  File \"/home/runner/work/joytishai/joytishai/tests/test_rag_quality.py\", line 87, in test_ragas_full_audit\n    pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/_pytest/outcomes.py\", line 193, in __call__\n    raise XFailed(msg=reason)\n","steps":[],"attachments":[{"uid":"89059ba2a67a25dc","name":"WARNING","source":"89059ba2a67a25dc.txt","type":"text/plain","size":27}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":true,"attachmentsCount":1,"hasContent":true,"attachmentStep":false}],"attachments":[{"uid":"f55596c096ab069e","name":"log","source":"f55596c096ab069e.txt","type":"text/plain","size":1681}],"parameters":[],"stepsCount":5,"shouldDisplayMessage":true,"attachmentsCount":5,"hasContent":true,"attachmentStep":false},"afterStages":[{"name":"_function_scoped_runner::0","time":{"start":1769185063611,"stop":1769185063612,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"feature","value":"RAG System Quality"},{"name":"story","value":"Accuracy and Retrieval Audit"},{"name":"epic","value":"Astrological AI"},{"name":"tag","value":"asyncio"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_rag_quality"},{"name":"subSuite","value":"TestJoytishRagas"},{"name":"host","value":"runnervmymu0l"},{"name":"thread","value":"3261-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_rag_quality"},{"name":"resultFormat","value":"allure2"}],"parameters":[{"name":"Answer Relevancy","value":"0.3085855956376061"},{"name":"Context Precision","value":"0.0"},{"name":"Knowledge Faithfulness","value":"0.08333333333333333"},{"name":"log_index","value":"4"}],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[],"tags":["asyncio"]},"source":"3a6da6db77d168fc.json","parameterValues":["0.3085855956376061","0.0","0.08333333333333333","4"]}