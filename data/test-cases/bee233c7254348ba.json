{"uid":"bee233c7254348ba","name":"test_2_ai_generation_logic","fullName":"tests.test_api.TestAstroEngineAPI#test_2_ai_generation_logic","historyId":"e51fcb7b3c9625416167fd3c0872e54e","time":{"start":1769167845324,"stop":1769167849505,"duration":4181},"description":"Check Internal AI Logic (Integration Test).","descriptionHtml":"<p>Check Internal AI Logic (Integration Test).</p>\n","status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","statusTrace":"self = <tests.test_api.TestAstroEngineAPI object at 0x7f330af72390>\n\n    @allure.story(\"JOYTISHAI\")\n    @allure.description(\"Check Internal AI Logic (Integration Test).\")\n    async def test_2_ai_generation_logic(self):\n        \"\"\"TEST 2: Check Internal AI Logic (Unit Test).\"\"\"\n        generator = AIEngine()\n        with allure.step(\"Checking AI connection using AI Engine\"):\n>           result = await generator.generate_consultation(payload_ai)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_api.py:295: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \napp/services/ai_engine.py:154: in generate_consultation\n    response = await chain.ainvoke(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py:3191: in ainvoke\n    input_ = await coro_with_context(part(), context, create_task=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py:5570: in ainvoke\n    return await self.bound.ainvoke(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421: in ainvoke\n    llm_result = await self.agenerate_prompt(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1128: in agenerate_prompt\n    return await self.agenerate(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1086: in agenerate\n    raise exceptions[0]\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1339: in _agenerate_with_cache\n    result = await self._agenerate(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1628: in _agenerate\n    raise e\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1591: in _agenerate\n    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1670: in parse\n    return await self._post(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1797: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.AsyncOpenAI object at 0x7f32baf1d950>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Helper-Method': 'chat.com...': False}, 'name': 'AstrologicalConsultation', 'strict': True}}, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n                raise APIConnectionError(request=request) from err\n    \n            log.debug(\n                'HTTP Response: %s %s \"%i %s\" %s',\n                request.method,\n                request.url,\n                response.status_code,\n                response.reason_phrase,\n                response.headers,\n            )\n            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n            try:\n                response.raise_for_status()\n            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n                if remaining_retries > 0 and self._should_retry(err.response):\n                    await err.response.aclose()\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=response,\n                    )\n                    continue\n    \n                # If the response is streamed then we need to explicitly read the response\n                # to completion before attempting to access the response text.\n                if not err.response.is_closed:\n                    await err.response.aread()\n    \n                log.debug(\"Re-raising status error\")\n>               raise self._make_status_error_from_response(err.response) from None\nE               openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1597: RateLimitError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1769167829034,"stop":1769167829034,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"_function_scoped_runner","time":{"start":1769167845323,"stop":1769167845323,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"description":"Check Internal AI Logic (Integration Test).","status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","statusTrace":"self = <tests.test_api.TestAstroEngineAPI object at 0x7f330af72390>\n\n    @allure.story(\"JOYTISHAI\")\n    @allure.description(\"Check Internal AI Logic (Integration Test).\")\n    async def test_2_ai_generation_logic(self):\n        \"\"\"TEST 2: Check Internal AI Logic (Unit Test).\"\"\"\n        generator = AIEngine()\n        with allure.step(\"Checking AI connection using AI Engine\"):\n>           result = await generator.generate_consultation(payload_ai)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_api.py:295: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \napp/services/ai_engine.py:154: in generate_consultation\n    response = await chain.ainvoke(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py:3191: in ainvoke\n    input_ = await coro_with_context(part(), context, create_task=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py:5570: in ainvoke\n    return await self.bound.ainvoke(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421: in ainvoke\n    llm_result = await self.agenerate_prompt(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1128: in agenerate_prompt\n    return await self.agenerate(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1086: in agenerate\n    raise exceptions[0]\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1339: in _agenerate_with_cache\n    result = await self._agenerate(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1628: in _agenerate\n    raise e\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1591: in _agenerate\n    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_legacy_response.py:381: in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1670: in parse\n    return await self._post(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1797: in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.AsyncOpenAI object at 0x7f32baf1d950>\ncast_to = <class 'openai.types.chat.chat_completion.ChatCompletion'>\noptions = FinalRequestOptions(method='post', url='/chat/completions', params={}, headers={'X-Stainless-Helper-Method': 'chat.com...': False}, 'name': 'AstrologicalConsultation', 'strict': True}}, 'stream': False, 'temperature': 0.3}, extra_json=None)\n\n    async def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_AsyncStreamT] | None = None,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = await self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            await self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = await self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n                raise APIConnectionError(request=request) from err\n    \n            log.debug(\n                'HTTP Response: %s %s \"%i %s\" %s',\n                request.method,\n                request.url,\n                response.status_code,\n                response.reason_phrase,\n                response.headers,\n            )\n            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n            try:\n                response.raise_for_status()\n            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n                if remaining_retries > 0 and self._should_retry(err.response):\n                    await err.response.aclose()\n                    await self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=response,\n                    )\n                    continue\n    \n                # If the response is streamed then we need to explicitly read the response\n                # to completion before attempting to access the response text.\n                if not err.response.is_closed:\n                    await err.response.aread()\n    \n                log.debug(\"Re-raising status error\")\n>               raise self._make_status_error_from_response(err.response) from None\nE               openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1597: RateLimitError","steps":[{"name":"Checking AI connection using AI Engine","time":{"start":1769167845367,"stop":1769167849502,"duration":4135},"status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n","statusTrace":"  File \"/home/runner/work/joytishai/joytishai/tests/test_api.py\", line 295, in test_2_ai_generation_logic\n    result = await generator.generate_consultation(payload_ai)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/joytishai/joytishai/app/services/ai_engine.py\", line 154, in generate_consultation\n    response = await chain.ainvoke(\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3191, in ainvoke\n    input_ = await coro_with_context(part(), context, create_task=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5570, in ainvoke\n    return await self.bound.ainvoke(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 421, in ainvoke\n    llm_result = await self.agenerate_prompt(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1128, in agenerate_prompt\n    return await self.agenerate(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1086, in agenerate\n    raise exceptions[0]\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1339, in _agenerate_with_cache\n    result = await self._agenerate(\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1628, in _agenerate\n    raise e\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1591, in _agenerate\n    raw_response = await self.root_async_client.chat.completions.with_raw_response.parse(  # noqa: E501\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_legacy_response.py\", line 381, in wrapped\n    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1670, in parse\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py\", line 1797, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py\", line 1597, in request\n    raise self._make_status_error_from_response(err.response) from None\n","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":true,"attachmentsCount":0,"hasContent":true,"attachmentStep":false}],"attachments":[{"uid":"39d605cdd9b77d7e","name":"log","source":"39d605cdd9b77d7e.txt","type":"text/plain","size":1157}],"parameters":[],"stepsCount":1,"shouldDisplayMessage":true,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},"afterStages":[{"name":"_function_scoped_runner::0","time":{"start":1769167849837,"stop":1769167849838,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"story","value":"JOYTISHAI"},{"name":"feature","value":"API"},{"name":"tag","value":"asyncio"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_api"},{"name":"subSuite","value":"TestAstroEngineAPI"},{"name":"host","value":"runnervmymu0l"},{"name":"thread","value":"2976-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_api"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":["asyncio"]},"source":"bee233c7254348ba.json","parameterValues":[]}