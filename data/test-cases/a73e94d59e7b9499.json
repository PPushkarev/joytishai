{"uid":"a73e94d59e7b9499","name":"test_6_knowledge_base_multi_source","fullName":"tests.test_rag.TestRAGIntegration#test_6_knowledge_base_multi_source","historyId":"7ac1606c27ea6f1943fa61b7a2b76049","time":{"start":1769167861829,"stop":1769167863279,"duration":1450},"description":"Validates retrieval from multiple PDF sources simultaneously.","descriptionHtml":"<p>Validates retrieval from multiple PDF sources simultaneously.</p>\n","status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","statusTrace":"self = <tests.test_rag.TestRAGIntegration object at 0x7f330afd5790>\n\n    @allure.story(\"Semantic Search\")\n    @allure.description(\"Validates retrieval from multiple PDF sources simultaneously.\")\n    async def test_6_knowledge_base_multi_source(self):\n        \"\"\"TEST 6: Multi-Source Retrieval Test.\"\"\"\n    \n        with allure.step(\"Initializing Vector Store Manager\"):\n            vsm = VectorStoreManager()\n            retriever = vsm.get_retriever()\n            retriever.search_kwargs = {\"k\": 15}\n    \n        query = \"What are the effects of Rahu and planetary periods (Dashas)?\"\n    \n        with allure.step(f\"Sending Semantic Query: {query}\"):\n>           docs = await retriever.ainvoke(query)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_rag.py:135: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/retrievers.py:277: in ainvoke\n    result = await self._aget_relevant_documents(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:1065: in _aget_relevant_documents\n    docs = await self.vectorstore.asimilarity_search(query, **kwargs_)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:621: in asimilarity_search\n    return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py:608: in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/concurrent/futures/thread.py:58: in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py:599: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:350: in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:439: in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:759: in embed_query\n    return self.embed_documents([text], **kwargs)[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:709: in embed_documents\n    return self._get_len_safe_embeddings(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:576: in _get_len_safe_embeddings\n    response = self.client.create(input=batch_tokens, **client_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/embeddings.py:132: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1259: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f32a6ec63d0>\ncast_to = <class 'openai.types.create_embedding_response.CreateEmbeddingResponse'>\noptions = FinalRequestOptions(method='post', url='/embeddings', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT...938, 18852, 320, 43363, 300, 12106]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}, extra_json=None)\n\n    def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_StreamT] | None = None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n                raise APIConnectionError(request=request) from err\n    \n            log.debug(\n                'HTTP Response: %s %s \"%i %s\" %s',\n                request.method,\n                request.url,\n                response.status_code,\n                response.reason_phrase,\n                response.headers,\n            )\n            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n            try:\n                response.raise_for_status()\n            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n                if remaining_retries > 0 and self._should_retry(err.response):\n                    err.response.close()\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=response,\n                    )\n                    continue\n    \n                # If the response is streamed then we need to explicitly read the response\n                # to completion before attempting to access the response text.\n                if not err.response.is_closed:\n                    err.response.read()\n    \n                log.debug(\"Re-raising status error\")\n>               raise self._make_status_error_from_response(err.response) from None\nE               openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1047: RateLimitError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1769167829034,"stop":1769167829034,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"_function_scoped_runner","time":{"start":1769167861827,"stop":1769167861829,"duration":2},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"description":"Validates retrieval from multiple PDF sources simultaneously.","status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","statusTrace":"self = <tests.test_rag.TestRAGIntegration object at 0x7f330afd5790>\n\n    @allure.story(\"Semantic Search\")\n    @allure.description(\"Validates retrieval from multiple PDF sources simultaneously.\")\n    async def test_6_knowledge_base_multi_source(self):\n        \"\"\"TEST 6: Multi-Source Retrieval Test.\"\"\"\n    \n        with allure.step(\"Initializing Vector Store Manager\"):\n            vsm = VectorStoreManager()\n            retriever = vsm.get_retriever()\n            retriever.search_kwargs = {\"k\": 15}\n    \n        query = \"What are the effects of Rahu and planetary periods (Dashas)?\"\n    \n        with allure.step(f\"Sending Semantic Query: {query}\"):\n>           docs = await retriever.ainvoke(query)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/test_rag.py:135: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/retrievers.py:277: in ainvoke\n    result = await self._aget_relevant_documents(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:1065: in _aget_relevant_documents\n    docs = await self.vectorstore.asimilarity_search(query, **kwargs_)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:621: in asimilarity_search\n    return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py:608: in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/concurrent/futures/thread.py:58: in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py:599: in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:350: in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:439: in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:759: in embed_query\n    return self.embed_documents([text], **kwargs)[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:709: in embed_documents\n    return self._get_len_safe_embeddings(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:576: in _get_len_safe_embeddings\n    response = self.client.create(input=batch_tokens, **client_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/embeddings.py:132: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1259: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f32a6ec63d0>\ncast_to = <class 'openai.types.create_embedding_response.CreateEmbeddingResponse'>\noptions = FinalRequestOptions(method='post', url='/embeddings', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT...938, 18852, 320, 43363, 300, 12106]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}, extra_json=None)\n\n    def request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool = False,\n        stream_cls: type[_StreamT] | None = None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n        if input_options.idempotency_key is None and input_options.method.lower() != \"get\":\n            # ensure the idempotency key is reused between requests\n            input_options.idempotency_key = self._idempotency_key()\n    \n        response: httpx.Response | None = None\n        max_retries = input_options.get_max_retries(self.max_retries)\n    \n        retries_taken = 0\n        for retries_taken in range(max_retries + 1):\n            options = model_copy(input_options)\n            options = self._prepare_options(options)\n    \n            remaining_retries = max_retries - retries_taken\n            request = self._build_request(options, retries_taken=retries_taken)\n            self._prepare_request(request)\n    \n            kwargs: HttpxSendArgs = {}\n            if self.custom_auth is not None:\n                kwargs[\"auth\"] = self.custom_auth\n    \n            if options.follow_redirects is not None:\n                kwargs[\"follow_redirects\"] = options.follow_redirects\n    \n            log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n            response = None\n            try:\n                response = self._client.send(\n                    request,\n                    stream=stream or self._should_stream_response_body(request=request),\n                    **kwargs,\n                )\n            except httpx.TimeoutException as err:\n                log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising timeout error\")\n                raise APITimeoutError(request=request) from err\n            except Exception as err:\n                log.debug(\"Encountered Exception\", exc_info=True)\n    \n                if remaining_retries > 0:\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=None,\n                    )\n                    continue\n    \n                log.debug(\"Raising connection error\")\n                raise APIConnectionError(request=request) from err\n    \n            log.debug(\n                'HTTP Response: %s %s \"%i %s\" %s',\n                request.method,\n                request.url,\n                response.status_code,\n                response.reason_phrase,\n                response.headers,\n            )\n            log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n            try:\n                response.raise_for_status()\n            except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n                log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n                if remaining_retries > 0 and self._should_retry(err.response):\n                    err.response.close()\n                    self._sleep_for_retry(\n                        retries_taken=retries_taken,\n                        max_retries=max_retries,\n                        options=input_options,\n                        response=response,\n                    )\n                    continue\n    \n                # If the response is streamed then we need to explicitly read the response\n                # to completion before attempting to access the response text.\n                if not err.response.is_closed:\n                    err.response.read()\n    \n                log.debug(\"Re-raising status error\")\n>               raise self._make_status_error_from_response(err.response) from None\nE               openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n\n/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py:1047: RateLimitError","steps":[{"name":"Initializing Vector Store Manager","time":{"start":1769167861829,"stop":1769167861875,"duration":46},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"Sending Semantic Query: What are the effects of Rahu and planetary periods (Dashas)?","time":{"start":1769167861875,"stop":1769167863277,"duration":1402},"status":"broken","statusMessage":"openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n","statusTrace":"  File \"/home/runner/work/joytishai/joytishai/tests/test_rag.py\", line 135, in test_6_knowledge_base_multi_source\n    docs = await retriever.ainvoke(query)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 277, in ainvoke\n    result = await self._aget_relevant_documents(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1065, in _aget_relevant_documents\n    docs = await self.vectorstore.asimilarity_search(query, **kwargs_)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 621, in asimilarity_search\n    return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 608, in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 599, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py\", line 350, in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py\", line 439, in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 759, in embed_query\n    return self.embed_documents([text], **kwargs)[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 709, in embed_documents\n    return self._get_len_safe_embeddings(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 576, in _get_len_safe_embeddings\n    response = self.client.create(input=batch_tokens, **client_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/resources/embeddings.py\", line 132, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\n","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":true,"attachmentsCount":0,"hasContent":true,"attachmentStep":false}],"attachments":[{"uid":"17d3f38045e3d929","name":"log","source":"17d3f38045e3d929.txt","type":"text/plain","size":563}],"parameters":[],"stepsCount":2,"shouldDisplayMessage":true,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},"afterStages":[{"name":"_function_scoped_runner::0","time":{"start":1769167863360,"stop":1769167863361,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"feature","value":"RAG Integration"},{"name":"story","value":"Semantic Search"},{"name":"tag","value":"asyncio"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_rag"},{"name":"subSuite","value":"TestRAGIntegration"},{"name":"host","value":"runnervmymu0l"},{"name":"thread","value":"2976-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_rag"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":["asyncio"]},"source":"a73e94d59e7b9499.json","parameterValues":[]}