{"uid":"84af50d26efda57","name":"Log Audit: 69739e058a6a8e39f50e25e0","fullName":"tests.test_rag_quality.TestJoytishRagas#test_ragas_full_audit","historyId":"ccec93318f416f1cbaa8a2c3dded8ecf","time":{"start":1769184914578,"stop":1769185025766,"duration":111188},"description":"User Query: name='CoT Tester' date='1990-01-01' time='12:00' city='London' latitude=51.5 longitude=-0.12 timezone='Europe/London' utc_offset=0.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}","descriptionHtml":"<p>User Query: name='CoT Tester' date='1990-01-01' time='12:00' city='London' latitude=51.5 longitude=-0.12 timezone='Europe/London' utc_offset=0.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}</p>\n","status":"skipped","statusMessage":"XFAIL Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.\n\n_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.","statusTrace":"self = <tests.test_rag_quality.TestJoytishRagas object at 0x7f6a27142c10>\nlog_index = 3\n\n    @allure.story(\"Accuracy and Retrieval Audit\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"log_index\", range(5))\n    async def test_ragas_full_audit(self, log_index):\n        \"\"\"\n        Professional Log Audit:\n        1. Verifying compliance with planetary data (Technical)\n        2. Verifying compliance with the PDF knowledge base (Knowledge)\n        \"\"\"\n        # Get URI from environment variables\n        mongo_uri = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n    \n        # 1. Load data\n        with allure.step(\"Loading data from MongoDB\"):\n            ds_tech, ds_know, logs, _ = await prepare_ragas_datasets(mongo_uri)\n    \n            if not logs or log_index >= len(logs):\n                pytest.skip(f\"Log with index {log_index} is missing in the database (pending)\")\n    \n            current_log = logs[log_index]\n            log_id = str(current_log[\"_id\"])\n            allure.dynamic.title(f\"Log Audit: {log_id}\")\n            allure.dynamic.description(f\"User Query: {current_log.get('user_query')}\")\n    \n        # 2. Technical Check (Planetary positions)\n        with allure.step(\"Technical Audit: Planet Accuracy\"):\n            res_t = evaluate(ds_tech.select([log_index]), metrics=[faithfulness],\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n            tech_score = float(res_t[\"faithfulness\"].iloc[0])\n    \n            allure.attach(f\"Score: {tech_score}\", name=\"Technical Faithfulness\",\n                          attachment_type=allure.attachment_type.TEXT)\n            # If technical accuracy is critical, you can enforce it:\n            # assert tech_score >= 0.5, \"The bot hallucinated planetary positions!\"\n    \n        # 3. Content Check (RAG)\n        with allure.step(\"Content Audit: PDF Knowledge Base\"):\n            metrics = [faithfulness, answer_relevancy, context_precision]\n            res_k = evaluate(ds_know.select([log_index]), metrics=metrics,\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n    \n            f_score = float(res_k[\"faithfulness\"].iloc[0])\n            r_score = float(res_k[\"answer_relevancy\"].iloc[0])\n            p_score = float(res_k[\"context_precision\"].iloc[0])\n    \n            # Pass metrics to Allure as parameters for dynamic charts\n            allure.dynamic.parameter(\"Knowledge Faithfulness\", f_score)\n            allure.dynamic.parameter(\"Context Precision\", p_score)\n            allure.dynamic.parameter(\"Answer Relevancy\", r_score)\n    \n            allure.attach(res_k.to_json(orient=\"records\"), name=\"Detailed Metrics JSON\",\n                          attachment_type=allure.attachment_type.JSON)\n    \n        # 4. Text Analysis (Comparison)\n        with allure.step(\"Comparison: Database Context vs AI Answer\"):\n            comparison_text = (\n                f\"QUERY: {current_log.get('user_query')}\\n\\n\"\n                f\"AI ANSWER: {ds_know.select([log_index])['answer'][0]}\\n\\n\"\n                f\"RETRIEVED CONTEXT: {ds_know.select([log_index])['contexts'][0]}\\n\\n\"\n                f\"REFERENCE: {ds_know.select([log_index])['reference'][0]}\"\n            )\n            allure.attach(comparison_text, name=\"Context & Answer Comparison\",\n                          attachment_type=allure.attachment_type.TEXT)\n    \n        # 5. Final Quality Gate (Soft Fail)\n        with allure.step(\"Final Verdict\"):\n            if f_score < 0.1:\n                # IF score less then 0.1 it is a fail\n                allure.attach(\"RAG Hallucination detected.\", name=\"WARNING\",\n                              attachment_type=allure.attachment_type.TEXT)\n>               pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\nE               _pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.\n\ntests/test_rag_quality.py:87: XFailed","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1769184833273,"stop":1769184833273,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"_function_scoped_runner","time":{"start":1769184914578,"stop":1769184914578,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"description":"User Query: name='CoT Tester' date='1990-01-01' time='12:00' city='London' latitude=51.5 longitude=-0.12 timezone='Europe/London' utc_offset=0.0 julian_day=2447893.0 lagna=0.0 sign='Aries' planets={}","status":"skipped","statusMessage":"XFAIL Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.\n\n_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.","statusTrace":"self = <tests.test_rag_quality.TestJoytishRagas object at 0x7f6a27142c10>\nlog_index = 3\n\n    @allure.story(\"Accuracy and Retrieval Audit\")\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"log_index\", range(5))\n    async def test_ragas_full_audit(self, log_index):\n        \"\"\"\n        Professional Log Audit:\n        1. Verifying compliance with planetary data (Technical)\n        2. Verifying compliance with the PDF knowledge base (Knowledge)\n        \"\"\"\n        # Get URI from environment variables\n        mongo_uri = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n    \n        # 1. Load data\n        with allure.step(\"Loading data from MongoDB\"):\n            ds_tech, ds_know, logs, _ = await prepare_ragas_datasets(mongo_uri)\n    \n            if not logs or log_index >= len(logs):\n                pytest.skip(f\"Log with index {log_index} is missing in the database (pending)\")\n    \n            current_log = logs[log_index]\n            log_id = str(current_log[\"_id\"])\n            allure.dynamic.title(f\"Log Audit: {log_id}\")\n            allure.dynamic.description(f\"User Query: {current_log.get('user_query')}\")\n    \n        # 2. Technical Check (Planetary positions)\n        with allure.step(\"Technical Audit: Planet Accuracy\"):\n            res_t = evaluate(ds_tech.select([log_index]), metrics=[faithfulness],\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n            tech_score = float(res_t[\"faithfulness\"].iloc[0])\n    \n            allure.attach(f\"Score: {tech_score}\", name=\"Technical Faithfulness\",\n                          attachment_type=allure.attachment_type.TEXT)\n            # If technical accuracy is critical, you can enforce it:\n            # assert tech_score >= 0.5, \"The bot hallucinated planetary positions!\"\n    \n        # 3. Content Check (RAG)\n        with allure.step(\"Content Audit: PDF Knowledge Base\"):\n            metrics = [faithfulness, answer_relevancy, context_precision]\n            res_k = evaluate(ds_know.select([log_index]), metrics=metrics,\n                             llm=judge_llm, embeddings=judge_embeddings).to_pandas()\n    \n            f_score = float(res_k[\"faithfulness\"].iloc[0])\n            r_score = float(res_k[\"answer_relevancy\"].iloc[0])\n            p_score = float(res_k[\"context_precision\"].iloc[0])\n    \n            # Pass metrics to Allure as parameters for dynamic charts\n            allure.dynamic.parameter(\"Knowledge Faithfulness\", f_score)\n            allure.dynamic.parameter(\"Context Precision\", p_score)\n            allure.dynamic.parameter(\"Answer Relevancy\", r_score)\n    \n            allure.attach(res_k.to_json(orient=\"records\"), name=\"Detailed Metrics JSON\",\n                          attachment_type=allure.attachment_type.JSON)\n    \n        # 4. Text Analysis (Comparison)\n        with allure.step(\"Comparison: Database Context vs AI Answer\"):\n            comparison_text = (\n                f\"QUERY: {current_log.get('user_query')}\\n\\n\"\n                f\"AI ANSWER: {ds_know.select([log_index])['answer'][0]}\\n\\n\"\n                f\"RETRIEVED CONTEXT: {ds_know.select([log_index])['contexts'][0]}\\n\\n\"\n                f\"REFERENCE: {ds_know.select([log_index])['reference'][0]}\"\n            )\n            allure.attach(comparison_text, name=\"Context & Answer Comparison\",\n                          attachment_type=allure.attachment_type.TEXT)\n    \n        # 5. Final Quality Gate (Soft Fail)\n        with allure.step(\"Final Verdict\"):\n            if f_score < 0.1:\n                # IF score less then 0.1 it is a fail\n                allure.attach(\"RAG Hallucination detected.\", name=\"WARNING\",\n                              attachment_type=allure.attachment_type.TEXT)\n>               pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\nE               _pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.\n\ntests/test_rag_quality.py:87: XFailed","steps":[{"name":"Loading data from MongoDB","time":{"start":1769184914578,"stop":1769184914595,"duration":17},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"Technical Audit: Planet Accuracy","time":{"start":1769184914595,"stop":1769184959320,"duration":44725},"status":"passed","steps":[],"attachments":[{"uid":"673f0f92fb5738e1","name":"Technical Faithfulness","source":"673f0f92fb5738e1.txt","type":"text/plain","size":10}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Content Audit: PDF Knowledge Base","time":{"start":1769184959320,"stop":1769185025761,"duration":66441},"status":"passed","steps":[],"attachments":[{"uid":"be88d0cb7b380816","name":"Detailed Metrics JSON","source":"be88d0cb7b380816.json","type":"application/json","size":40718}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Comparison: Database Context vs AI Answer","time":{"start":1769185025761,"stop":1769185025765,"duration":4},"status":"passed","steps":[],"attachments":[{"uid":"e6305c8352e8a391","name":"Context & Answer Comparison","source":"e6305c8352e8a391.txt","type":"text/plain","size":14731}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},{"name":"Final Verdict","time":{"start":1769185025765,"stop":1769185025765,"duration":0},"status":"failed","statusMessage":"_pytest.outcomes.XFailed: Soft Fail: Quality is too low (0.05555555555555555), but we don't block the pipeline.\n","statusTrace":"  File \"/home/runner/work/joytishai/joytishai/tests/test_rag_quality.py\", line 87, in test_ragas_full_audit\n    pytest.xfail(f\"Soft Fail: Quality is too low ({f_score}), but we don't block the pipeline.\")\n  File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/_pytest/outcomes.py\", line 193, in __call__\n    raise XFailed(msg=reason)\n","steps":[],"attachments":[{"uid":"4a48be8ac8ad1a28","name":"WARNING","source":"4a48be8ac8ad1a28.txt","type":"text/plain","size":27}],"parameters":[],"stepsCount":0,"shouldDisplayMessage":true,"attachmentsCount":1,"hasContent":true,"attachmentStep":false}],"attachments":[{"uid":"96af8de4f5b52e9c","name":"log","source":"96af8de4f5b52e9c.txt","type":"text/plain","size":1789}],"parameters":[],"stepsCount":5,"shouldDisplayMessage":true,"attachmentsCount":5,"hasContent":true,"attachmentStep":false},"afterStages":[{"name":"_function_scoped_runner::0","time":{"start":1769185025771,"stop":1769185025772,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"shouldDisplayMessage":false,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"feature","value":"RAG System Quality"},{"name":"story","value":"Accuracy and Retrieval Audit"},{"name":"epic","value":"Astrological AI"},{"name":"tag","value":"asyncio"},{"name":"parentSuite","value":"tests"},{"name":"suite","value":"test_rag_quality"},{"name":"subSuite","value":"TestJoytishRagas"},{"name":"host","value":"runnervmymu0l"},{"name":"thread","value":"3261-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"tests.test_rag_quality"},{"name":"resultFormat","value":"allure2"}],"parameters":[{"name":"Answer Relevancy","value":"0.30298064502268807"},{"name":"Context Precision","value":"0.0"},{"name":"Knowledge Faithfulness","value":"0.05555555555555555"},{"name":"log_index","value":"3"}],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[],"tags":["asyncio"]},"source":"84af50d26efda57.json","parameterValues":["0.30298064502268807","0.0","0.05555555555555555","3"]}